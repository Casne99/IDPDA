\documentclass[a4paper,12pt]{report}

\usepackage[utf8]{inputenc}     % Per caratteri accentati
\usepackage[T1]{fontenc}        % Font encoding
\usepackage[italian]{babel}     % Lingua italiana
\usepackage{lmodern}            % Font leggibile
\usepackage{geometry}           % Margini personalizzati
\geometry{margin=3cm}
\usepackage{setspace}           % Spaziatura
\onehalfspacing                 % Interlinea 1.5
\usepackage{graphicx}           % Immagini
\usepackage{float}              % Controllo posizionamento figure
\usepackage{amsmath, amssymb}   % Matematica
\usepackage{amsthm}             % Ambienti per teoremi, definizioni, ecc.
\usepackage{hyperref}           % Link ipertestuali
\usepackage{fancyhdr}           % Intestazioni e piè di pagina
\usepackage{titlesec}
\usepackage{mathtools}           % Formattazione titoli
\usepackage{tikz}                % Per diagrammi TikZ
\usetikzlibrary{automata,positioning,calc,circuits.logic.US}
\usepackage{algorithm}           % Per algoritmi
\usepackage{algpseudocode}       % Per pseudocodice

% Definizione ambiente teorema numerato per capitolo
\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{definition}{Definizione}[chapter]

% Personalizzazione stile proposizione
\newtheoremstyle{propositionstyle}
{12pt}   % spazio sopra
{12pt}   % spazio sotto
{\itshape} % font del corpo
{0pt}    % indentazione
{\bfseries} % font dell'intestazione
{.}      % punteggiatura dopo l'intestazione
{.5em}   % spazio dopo l'intestazione
{}       % specifica dell'intestazione

\theoremstyle{propositionstyle}
\newtheorem{prop}{Proposizione}[chapter]


\titleformat{\chapter}[display]
{\normalfont\huge\bfseries}
{}
{0pt}
{\Huge}

% Impostazioni intestazioni
\pagestyle{fancy}
\fancyhead{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[RE]{\leftmark}
\fancyhead[LO]{\rightmark}

% Frontespizio personalizzabile
\begin{document}

    \begin{titlepage}
        \centering
        {\scshape\LARGE Università degli Studi di Milano \par}
        \vspace{1cm}
        {\scshape\Large Facoltà di Scienze e Tecnologie \par}
        \vspace{0.5cm}
        {\scshape\large Corso di Laurea in Informatica\par}
        \vspace{2cm}
        {\huge\bfseries Input Driven Pushdown Automata ed Edit Distances\par}
        \vspace{2cm}
        \begin{flushleft}
            \textbf{Relatore:} Giovanni Pighizzini\\
            \textbf{Candidato:} Luca Casnedi\\
            \textbf{Matricola:} 931856
        \end{flushleft}
        \vfill
        {\large Anno Accademico 2024--2025\par}
    \end{titlepage}

    \pagenumbering{Roman}
    \setcounter{tocdepth}{3}
    \tableofcontents
    \clearpage

    \pagenumbering{arabic}


    \chapter{Introduzione}


    \section{Automi}

    Gli \emph{Input Driven Pushdown Automata}, anche detti IDPDA, sono una classe di automi a pila in cui le operazioni sulla pila sono esclusivamente determinate dal simbolo di input.
    In particolare, il simbolo di input determina se l'automa compierà un'operazione di \emph{push} sulla pila, di \emph{pop}, oppure se lascerà la pila invariata.

    Gli IDPDA risultano particolarmente utili in diversi ambiti, tra cui il \emph{parsing} dei linguaggi di programmazione, dei documenti XML e, più in generale, in tutti i contesti in cui la struttura dell'input riflette la struttura gerarchica dei dati.
    In particolare, gli IDPDA sono stati introdotti per riconoscere linguaggi che rispettano specifiche regole di parentesizzazione.

    In termini generali, un IDPDA si comporta come un automa a stati finiti (DFA); tuttavia, durante la lettura della stringa di input, la categoria a cui appartiene ciascun simbolo analizzato stabilisce in modo univoco se l’automa debba eseguire un’operazione di \textit{push}, di \textit{pop} oppure lasciare invariato il contenuto della pila.

    A differenza degli automi a stati finiti (DFA), che possono riconoscere solo linguaggi regolari, gli IDPDA sono in grado di gestire strutture annidate e bilanciate grazie alla presenza della pila.
    Gli automi a stati finiti, infatti, non possono riconoscere linguaggi come $\{a^n b^n \mid n \geq 0\}$ a causa della loro memoria limitata agli stati, mentre gli IDPDA possono facilmente gestire tali costrutti utilizzando la pila (e le operazioni \textit{push} e \textit{pop} ad essa associate) per tenere traccia delle informazioni sul bilanciamento.

    È interessante notare come la famiglia dei linguaggi riconosciuti dagli IDPDA mantenga molte delle proprietà tipiche dei linguaggi regolari, pur essendo più ampia.


    \section{Distanze}

    Un problema di particolare rilevanza, dove gli IDPDA trovano utilizzo, consiste nel determinare la distanza tra una data stringa $s$ e un linguaggio $L$, detta anche \textit{distanza di edit}.
    Tale distanza è definita come il costo minimo di una sequenza di operazioni elementari di modifica su $s$ (sostituzione, inserimento, eliminazione di un simbolo), che la trasformano in una stringa $s' \in L$.

    In uno scenario applicativo, si può supporre di aver ricevuto in input una stringa appartenente a $L$, la quale potrebbe tuttavia contenere errori.
    L'obiettivo è quindi ricostruire, con la miglior accuratezza possibile, la stringa originaria.

    Per ottenere tale risultato, si può adottare un approccio a \emph{distanza minima}, ovvero determinare la stringa $y \in L$ tale che la \textit{distanza di edit} tra $x$ (la stringa ricevuta) e $y$ sia minima.


    \chapter{Automi Input Driven}


    \section{Notazioni}

    Nel seguito, $\Sigma$ denota un alfabeto finito e $\Sigma^*$ è l'insieme delle stringhe su $\Sigma$, $\epsilon$ è la stringa vuota e la lunghezza di una stringa $w \in \Sigma^*$ è indicata con $|w|$. Dati gli insiemi $S$ e $T$, l'insieme delle funzioni parziali da $S$ a $T$ è denotato da $T^S$. La cardinalità di un insieme finito $S$ è $|S|$. Il simbolo $\perp$ denota la pila vuota e $2^S$ denota l'insieme delle parti dell'insieme $S$.


    \section{Input Driven Pushdown Automata}

    Una transizione di un IDPDA è determinata dal tipo di simbolo in input, appartenente all'alfabeto $\Sigma$, che stabilisce l'operazione da eseguire, eventualmente nulla.

    Si può rappresentare l'alfabeto di input di un IDPDA come unione di tre sottoinsiemi disgiunti e finiti:

    \begin{itemize}
        \item $\Sigma_{+1}$: parentesi sinistre, determinano un'operazione di caricamento sulla pila
        \item $\Sigma_{-1}$: parentesi destre, determinano un'operazione di pop sulla pila
        \item $\Sigma_{0}$: simboli neutri, non si traducono in operazioni sulla pila
    \end{itemize}

    Una stringa $s \in \Sigma^*$ è detta \textit{ben formata} se ogni parentesi sinistra ha una parentesi destra corrispondente, e viceversa.


    \section{Automi Input Driven Deterministici}

    \begin{definition}
        \label{def:didpda}
        Un \textit{automa a pila input driven deterministico} (detto anche DIDPDA) è una settupla $A = \left(\Sigma, Q, \Gamma, q_0, \perp, \left[ \delta_a \right]_{a \in \Sigma}, F\right)$ dove:
        \begin{itemize}
            \item $\Sigma$ è un alfabeto di input, composto da tre sotto-insiemi finiti e disgiunti $\Sigma_{+1}$, $\Sigma_{-1}$, $\Sigma_{0}$
            \item $Q$ è l'insieme di stati dell'automa
            \item $q_{0} \in Q$ è lo stato iniziale
            \item $F \subseteq Q$ è il sottoinsieme degli stati \textit{accettanti}
            \item $\Gamma$ è l'\textit{alfabeto della pila}
            \item $\perp$ è il simbolo che indica la pila vuota
            \item $\delta_{<} : Q \rightarrow Q \times \Gamma$ è la funzione di transizione che, per ogni parentesi sinistra $< \in \Sigma_{+1}$, dato un determinato stato corrente, fornisce lo stato successivo e il simbolo da caricare sulla pila
            \item $\delta_{>} : Q \times \left(\Gamma\;\cup\perp\right) \rightarrow Q$ è la funzione di transizione che, per ogni parentesi destra $> \in \Sigma_{-1}$, dato un determinato stato corrente, fornisce lo stato successivo, posto che il simbolo specificato si trovi in cima alla pila e venga rimosso da essa o lo stack sia vuoto
            \item $\delta_{c} : Q \rightarrow Q$ è la funzione di transizione che, per ogni simbolo neutro $c \in \Sigma_{0}$, fornisce il prossimo stato
        \end{itemize}
    \end{definition}

    Data questa definizione del modello comportamentale di un DIDPDA alla lettura di un simbolo, si può subito notare come il contenuto della pila
    sia preso in considerazione solo quando il simbolo di input è una parentesi destra.

    Quando il simbolo in input è, invece, un simbolo neutro o una parentesi sinistra, il prossimo stato non dipende dal contenuto dello stack.

    \begin{definition}
        \label{def:configuration}
        Una configurazione di $A$ è una tripla $\left(q, w, x\right)$ dove $q \in Q$ indica lo stato, $w \in \Sigma^*$ è l'input rimanente ed  $x \in \Gamma$ è il contenuto dello stack.
        La configurazione iniziale per una stringa $w_0 \in \Sigma^*$ è $\left(q_0, w_0, \epsilon\right)$.
        Per ogni configurazione con almeno un simbolo di input rimanente, la configurazione successiva è univocamente determinata da un singolo step della funzione di transizione:
        \begin{itemize}
            \item per ogni parentesi sinistra $< \in \Sigma_{+1}$ sia $(q, <w, x) \vdash_{A} (q', w, \gamma x)$, dove $\delta_{<}(q) = (q', \gamma)$
            \item per ogni parentesi destra $> \in \Sigma_{-1}$ sia $(q, >w, \gamma x) \vdash_{A} (\delta_{>}(q, \gamma), w, x)$, e nel caso lo stack fosse vuoto $(q, >w, \epsilon) \vdash_{A} (\delta_{>}(q, \perp), w, \epsilon)$
            \item per ogni simbolo neutro $c \in \Sigma_{0}$ sia $(q, cw, x) \vdash_{A} (\delta_{c}(q), w, x)$
        \end{itemize}

        Quando la stringa di input è stata letta, l'ultima configurazione $\left(q, \epsilon, u\right)$ è accettante se $q \in F$, a prescindere dal contenuto della pila.
        Il linguaggio $L\left(A\right)$ riconosciuto dall'automa è l'insieme di tutte le stringhe $w \in \Sigma^*$ per le quali la computazione che parte da $\left(q_0, w, \epsilon\right)$ è accettante.
    \end{definition}

    Un DIPDA può accettare stringhe con parentesi sinistre che non hanno una corrispondente parentesi destra, quello che si verifica al termine della computazione in questi casi è che la pila non è vuota.

    Le parentesi destre senza una corrispondente parentesi sinistra si possono trattare utilizzando il simbolo di pila vuota $\perp$ usato come secondo argomento della funzione di transizione, al posto di un simbolo dell'alfabeto della pila.

    Se l'alfabeto contiene solo simboli neutri, ovvero $\Sigma_{-1} = \Sigma_{+1} = \emptyset$, allora un DIPDA diventa un automa a stati deterministico (DFA).

    \clearpage

    \subsection{Esempio di DIDPDA}

    Per illustrare il funzionamento di un automa input driven deterministico, consideriamo un esempio.
    L'automa seguente riconosce stringhe bilanciate con simboli neutri interni:

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[>=stealth, shorten >=1pt, auto, node distance=4cm, semithick, font=\scriptsize, scale=1.5, transform shape]
            \tikzstyle{every state}=[fill=white, draw=black, text=black, minimum size=12mm]

            \node[state, accepting] (q0) {$q_{0}$};
            \node[state, accepting, right=4cm of q0] (q1) {$q_{1}$};
            \node[state, below=3cm of $(q0)!0.5!(q1)$] (qerr) {$q_{\text{err}}$};

            % freccia iniziale dall'alto
            \draw[->] ([yshift=1cm]q0.north) -- (q0.north);

            % archi di transizione
            \path[->]
            (q0) edge[loop below] node[below left] {$<$/push $X$} ()
            (q0) edge[bend left=15] node[above] {$a$} (q1)
            (q0) edge node[left] {$>$/err} (qerr)
            (q1) edge[loop above] node[above] {$a$} ()
            (q1) edge[bend left=15] node[below] {$>$/pop $X$} (q0)
            (q1) edge node[right] {$>$/err} (qerr);
        \end{tikzpicture}
        \caption{DIDPDA che riconosce stringhe bilanciate con simboli neutri}
        \label{fig:esempio_didpda}
    \end{figure}

    Per l'automa rappresentato in Figura~\ref{fig:esempio_didpda}, gli alfabeti sono:
    \begin{itemize}
        \item $\Sigma_{+1} = \{<\}$ (parentesi sinistre)
        \item $\Sigma_{-1} = \{>\}$ (parentesi destre)
        \item $\Sigma_0 = \{a\}$ (simboli neutri)
        \item $\Gamma = \{X\}$ (alfabeto della pila)
    \end{itemize}

    Le funzioni di transizione sono:
    \begin{align}
        \delta_{<}(q_0) &= (q_0, X) \\
        \delta_{a}(q_0) &= q_1 \\
        \delta_{a}(q_1) &= q_1 \\
        \delta_{>}(q_0, \gamma) &= q_{\text{err}} \\
        \delta_{>}(q_1, \gamma) &= \begin{cases}
                                       q_0 & \text{se } \gamma = X \\
                                       q_{\text{err}} & \text{se } \gamma = \perp
        \end{cases}
    \end{align}

    Questo automa accetta stringhe come $\epsilon$, $a$, $aa$, $<a>$, $<aa>$, $<a><aa>$, ma rifiuta stringhe come $>$, $<$, $<a$, $a>$, $<>$, $<<>>$.


    \section{Automi Input Driven non Deterministici}

    \begin{definition}
        Un \textit{automa a pila input driven non deterministico} (detto anche NIDPDA) è una settupla $A = \left(\Sigma, Q, \Gamma, q_0, \perp, \delta, F\right)$ dove:
        \begin{itemize}
            \item l'alfabeto di input $\Sigma$, l'insieme degli stati $Q$, l'alfabeto della pila $\Gamma$, lo stack vuoto $\perp$ e l'insieme degli stati accettanti $F \subseteq Q$ sono definiti come nella Definizione~\ref{def:didpda}
            \item esiste un insieme di stati iniziali $Q_0 \in Q$, ed una computazione potrebbe partire da ognuno di essi
            \item per ogni parentesi sinistra $< \in \Sigma_{+1}$, la funzione di transizione $\delta_{<} : Q \rightarrow 2^{Q \times \Gamma}$ fornisce, dato un certo stato attuale, un insieme di possibili risultati, ovvero coppie che indicano stato successivo e simbolo da caricare sullo stack
            \item per ogni parentesi destra $> \in \Sigma_{-1}$, la funzione di transizione $\delta_{>} : Q \times \left(\Gamma \cup \{\perp\}\right) \rightarrow 2^Q$, dato lo stato attuale e il simbolo in cima alla pila (che sarà rimosso se la pila non è vuota), fornisce un insieme di possibili stati successivi
            \item per ogni simbolo neutro $c \in \Sigma_{0}$, c'è una funzione $\delta_{0} : Q \rightarrow 2^Q$
        \end{itemize}
    \end{definition}

    \begin{definition}
        \label{def:configuration-non-det}
        Una configurazione di $A$ è una tripla $\left(q, w, x\right)$, con $q \in Q$, $w \in \Sigma^*$ ed $x \in \Gamma^*$.
        Presa una stringa di input $w_0$, tutte le configurazioni $(q_0, w_0, \epsilon)$ con $q_0 \in Q_0$ sono iniziali.
        La relazione di transizione è definita come segue:

        \begin{itemize}
            \item per ogni parentesi sinistra $< \in \Sigma_{+1}$, e per ogni coppia $\left(q', \gamma\right) \in \sigma_<\left(q\right)$, sia $(q, <w, x) \vdash_{A} (q', w, \gamma x)$;
            \item per ogni parentesi destra $> \in \Sigma_{-1}$ e per ogni $q' \in \sigma_>\left(q, \gamma\right)$, sia $(q, >w, \gamma x) \vdash_{A} (q', w, x)$, e nel caso lo stack fosse vuoto, per ogni $q' \in \sigma_>\left(q, \perp\right)$, sia $\left(q, >w, \epsilon\right) \vdash_{A} \left(q', w, \epsilon\right)$
            \item per un simbolo neutro $c \in \Sigma_0$, per ogni $q' \in \sigma_c\left(q\right)$, sia $\left(q, cw, x\right) \vdash_{A} \left(q', w, x\right)$
        \end{itemize}

        L'ultima configurazione $\left(q, \epsilon, u\right)$ è accettante se $q \in F$.
        Il linguaggio $L$ riconosciuto dall'automa è l'insieme di tutte le stringhe $w \in \Sigma^*$ per le quali almeno una computazione da $\left(q_0, w, \epsilon\right)$ è accettante.

    \end{definition}

    Un NIDPDA $B$ diventa un DIPDA se $\lvert Q_0\rvert = 1$ e le sue funzioni di transizione $\sigma_a$, $a \in \Sigma$ forniscono al più una azione possibile per ogni simbolo di input, stato e simbolo in cima alla pila.
    Se ci sono solo simboli neutri nell'alfabeto $\left(\Sigma_{+1} = \Sigma_{-1} = \emptyset\right)$, allora un NIDPDA è una automa a stati finiti non deterministico (NFA)

    \subsection{Esempio di NIDPDA}

    Consideriamo ora un esempio di NIDPDA. L'automa seguente riconosce stringhe bilanciate, che possono eventualmente contenere un simbolo neutro (una o più volte), anche alla loro fine:

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[>=stealth, shorten >=1pt, auto, node distance=3.5cm, semithick, font=\scriptsize, scale=1.4, transform shape]
            \tikzstyle{every state}=[fill=white, draw=black, text=black, minimum size=12mm]

            \node[state, accepting] (q0) {$q_{0}$};
            \node[state, above right=2cm and 3cm of q0] (q1) {$q_{1}$};
            \node[state, accepting, below right=2cm and 3cm of q1] (q2) {$q_{2}$};

            % freccia iniziale
            \draw[->] ([yshift=1cm]q0.north) -- (q0.north);

            % --- transizioni tra q0 e q1 con etichette inclinate e sotto la freccia ---
            \path[->]
            % q0 → q1
            (q0) edge[bend left=20, looseness=1.3] node[pos=0.4, sloped, below] {$($ / push $A$} (q1)
            (q0) edge[bend left=45, looseness=1.2] node[pos=0.6, sloped, below] {$[$ / push $B$} (q1)

            % q1 → q0
            (q1) edge[bend left=20, looseness=1.3] node[pos=0.4, sloped, below] {$)$ / pop $A$} (q0)
            (q1) edge[bend left=45, looseness=1.2] node[pos=0.6, sloped, below] {$]$ / pop $B$} (q0)

            % --- transizioni tra q1 e q2 (invariato) ---
            (q1) edge[bend right=15] node[below, sloped] {$)$/pop $A$} (q2)
            (q1) edge[bend left=15] node[above, sloped] {$]$/pop $B$} (q2)

            % loop su q2
            (q2) edge[loop right] node[right] {$a$} ();
        \end{tikzpicture}
        \caption{NIDPDA che riconosce stringhe bilanciate con scelte non deterministiche}
        \label{fig:nidpda_esempio}
    \end{figure}



    Per l'automa in Figura~\ref{fig:nidpda_esempio}, gli alfabeti sono:
    \begin{itemize}
        \item $\Sigma_{+1} = \{(, [\}$ (parentesi sinistre)
        \item $\Sigma_{-1} = \{), ]\}$ (parentesi destre)
        \item $\Sigma_0 = \{a\}$ (simboli neutri)
        \item $\Gamma = \{A, B\}$ (alfabeto della pila)
    \end{itemize}

    Le funzioni di transizione sono:
    \begin{align}
        \delta_{(}(q_0) &= \{(q_1, A)\} \\
        \delta_{[}(q_0) &= \{(q_1, B)\} \\
        \delta_{)}(q_1, A) &= \{q_0, q_2\} \\
        \delta_{]}(q_1, B) &= \{q_0, q_2\} \\
        \delta_{a}(q_2) &= \{q_2\}
    \end{align}

    Il non determinismo si manifesta quando l'automa legge una parentesi destra: può scegliere di tornare in $q_0$ (per continuare il bilanciamento) o andare in $q_2$ (per leggere simboli neutrali).
    Questo automa accetta stringhe come $()$, $[]$, $(a)$, $[aa]$, $()[]$, ma anche $()a$, $[]aa$.


    \section{Grammatiche Input-Driven}

    Gli Input Driven Pushdown Automata sono stati introdotti durante lo studio di particolari tipi di grammatiche, queste grammatiche sono definite
    con un alfabeto e una serie di regole che danno origine a stringhe che osservano vari criteri di \textit{corretta parentesizzazione}.

    Il primo dei modelli studiati sono state le \textit{grammatiche di parentesi} di McNaughton, nel quale l'alfabeto conteneva una parentesi sinistra $<$, una parentesi destra $>$
    ed una serie di simboli terminali, e le regole erano nella forma $X \rightarrow <\alpha>$, con $\alpha \in \left(\Sigma_0 \cup N\right)$ (dove $N$ è l'insieme dei simboli non terminali della grammatica), oppure nella forma $X \rightarrow c$ con $c \in \Sigma_0^*$.

    Tutte queste grammatiche vanno a dare origine a sottoclassi dei linguaggi input-driven.

    \begin{definition}
        Una grammatica input-driven è una quadrupla $G = \left(\Sigma, N, R, S\right)$ dove:
        \begin{itemize}
            \item $\Sigma = \Sigma_{-1} \cup \Sigma_0 \cup \Sigma_{+1}$ è l'alfabeto, diviso in tre sottoinsiemi disgiunti
            \item $N$ è l'insieme dei simboli non terminali
            \item $R$ è l'insieme delle regole, ognuna nella forma $A \rightarrow <B>C$, $A \rightarrow aC$, $A \rightarrow \epsilon$, dove $A, B, C \in N$, $< \in \Sigma_{+1}$, $> \in \Sigma_{-1}$, $a \in \Sigma$.
            \item $S$ è il simbolo iniziale
        \end{itemize}
    \end{definition}

    \subsection{Esempio di grammatica input-driven}

    Un modello più generale, studiato da Berstel e Boasson, sono le \textit{grammatiche bilanciate}, nelle quali esiste una corrispondenza biunivoca tra parentesi sinistre e destre, in modo che ogni regola $A \rightarrow <\alpha>$ debba utilizzare una coppia di parentesi corrispondenti.

    Consideriamo un esempio di grammatica bilanciata con:
    \begin{itemize}
        \item $\Sigma_{+1} = \{<, [\}$
        \item $\Sigma_{-1} = \{>, ]\}$
        \item $\Sigma_0 = \{\beta\}$
        \item $N = \{S, A\}$
    \end{itemize}

    Le regole di produzione possono essere definite come segue:

    \begin{align}
        S &\rightarrow < A > \mid [A] \mid \epsilon \\
        A &\rightarrow < A > \mid [A] \mid \beta \mid \epsilon
    \end{align}

    In questo esempio ogni regola utilizza una coppia di parentesi corrispondenti, rispettando il vincolo delle grammatiche bilanciate. La grammatica genera stringhe come $<>$, $<\beta>$, $[\beta]$, $<<\beta>>$, $[[\beta]]$, $<[\beta]>$, $[<\beta>]$.


    \chapter{Distanze e complessità}


    \section{Distanze di Edit}

    Il problema del calcolare le distanze di edit tra stringhe è strettamente correlato al problema del trovare la stringa con distanza di edit minima rispetto ad una data (correction problem).

    Una operazione di edit è una coppia $(a, b) \neq (\epsilon, \epsilon)$ con $|a|, |b| \leq 1$, ed è comunemente indicata con $a \rightarrow b$.

    Una certa stringa $y$ può essere derivata da un'altra stringa $x$ tramite una operazione di edit $a \rightarrow b$, se $\exists v, w \in \Sigma^* \mid x = vaw, y = vbw$.

    Dati $a, b \in \Sigma, a \neq b$, possiamo definire le operazioni $a \rightarrow b$, $a \rightarrow \epsilon$ e $\epsilon \rightarrow a$ rispettivamente
    \textit{sostituzione}, \textit{cancellazione} e \textit{inserimento}.

    Ad ogni operazione di edit è associato un costo, rappresentato mediante la funzione $\gamma(a \rightarrow b)$.

    Data una sequenza di operazioni di edit $S = \left(s_1, s_2, \dots, s_k\right)$, il costo di $S$ è definito come la somma dei costi delle singole operazioni di edit
    $\sum_{i=1}^{k} \gamma(s_i)$.

    Si assume che dati $x, y, k \in \left(\Sigma \cup \epsilon\right)$, $\gamma\left(x \rightarrow y\right) = 0$ se e solo se $x = y$, e $\gamma\left(x \rightarrow y\right)
    + \gamma\left(y \rightarrow k\right) \geq \gamma\left(x \rightarrow k\right)$.

    \begin{definition}
        Date due stringhe $x, y \in \Sigma^*$, la distanza di edit $d\left(x, y\right)$ da $x$ a $y$ è il minimo dei costi delle sequenze di operazioni di edit che trasformano $x$ in $y$.
    \end{definition}

    Ad esempio, sia $\Sigma = \{i, j\}$ e $\gamma\left(a \rightarrow b\right) = 1$ per ogni $a, b \in \Sigma \cup \{\epsilon\}$ con $a \neq b$. Per trasformare la stringa $iji$ nella stringa $jj$ si può utilizzare la seguente sequenza di operazioni:

    $$iji \xrightarrow{i \rightarrow j} jji \xrightarrow{i \rightarrow \epsilon} jj$$

    In questo caso sono state utilizzate una sostituzione e una cancellazione, per un costo totale di $1 + 1 = 2$.

    Consideriamo ora un esempio con un linguaggio infinito. Sia $L = \{a^n b^n \mid n \geq 1\}$ il linguaggio delle stringhe bilanciate su $\{a, b\}$ e la stringa $x = aabb$. Con la stessa funzione di costo $\gamma(c \rightarrow d) = 1$ per $c \neq d$, abbiamo $d(L, x) = 0$ poiché $x \in L$. Se invece consideriamo $x = aaabb$, possiamo trasformarla in $aabb \in L$ mediante:

    $$aaabb \xrightarrow{a \rightarrow \epsilon} aabb$$

    ottenendo $d(L, aaabb) = 1$.

    Questa definizione di distanza si può estendere, fissando un concetto di distanza che non riguarda due stringhe ma bensì una stringa e un linguaggio.

    \begin{definition}
        La distanza di edit tra un linguaggio $L \subseteq \Sigma^*$ ed una stringa $x \in \Sigma^*$ è il minimo tra i costi delle sequenze di operazioni di edit
        che trasformano stringhe di $L$ in $x$, ovvero $d\left(L, x\right) = \min\left\{d\left(y, x\right) \mid y \in L \right\}$.
    \end{definition}

    Ad esempio, dato $L = \{aab, aabb, abc\}$ ed $x = acb$, assumendo $\gamma(a \rightarrow b) = 1$ per ogni $a \neq b$, abbiamo che $d\left(L, x\right) = 1$, infatti $aab \xrightarrow{a \rightarrow c} acb$. Si noti che $x \in L \implies d\left(L, x\right) = 0$.

    Fissato un linguaggio $L \subseteq \Sigma^*$, la funzione $d\left(L, x\right)$ nella variabile $x$ viene denominata \textit{distanza di edit del linguaggio} $L$.

    \subsection{Esempio: distanza di Hamming}

    La \textit{distanza di Hamming} è definita solo per stringhe di uguale lunghezza e considera esclusivamente operazioni di sostituzione, ovvero le operazioni di cancellazione ed inserimento hanno costo infinito.

    Formalmente, data una funzione di costo generale $\gamma$, la distanza di Hamming si ottiene ponendo:
    \begin{align}
        \gamma(a \rightarrow b) &= \begin{cases}
                                       0 & \text{se } a = b \\
                                       1 & \text{se } a \neq b \text{ e } a, b \in \Sigma \\
                                       +\infty & \text{se } a = \epsilon \text{ o } b = \epsilon
        \end{cases}
    \end{align}

    Con questa definizione, per stringhe $x, y \in \Sigma^*$ con $|x| \neq |y|$, si ha $d_H(x, y) = +\infty$, mentre per $|x| = |y| = n$:
    \[
        d_H(x, y) = \sum_{i=1}^{n} \begin{cases}
                                       0 & \text{se } x_i = y_i \\
                                       1 & \text{se } x_i \neq y_i
        \end{cases}
    \]

    Ad esempio, consideriamo le stringhe $x = 1011$ e $y = 1101$ sull'alfabeto $\{0, 1\}$. La distanza di Hamming si calcola confrontando posizione per posizione:
    \begin{align*}
        d_H(1011, 1101) &= \sum_{i=0}^{3} \begin{cases}
                                              0 & \text{se } x_i = y_i \\
                                              1 & \text{se } x_i \neq y_i
        \end{cases} \\
        &= 0 + 1 + 1 + 0 = 2
    \end{align*}

    Infatti, le stringhe differiscono nelle posizioni 1 e 2: $x_1 = 0 \neq 1 = y_1$ e $x_2 = 1 \neq 0 = y_2$.

    \subsection{Calcolo delle distanze di edit}

    Date due stringhe, la distanza di edit tra di esse può essere determinata in termini di distanza tra le loro sottostringhe.

    \begin{prop}
        Siano $u,v,w,z \in \Sigma^*$. Allora vale
        \[
            d\left(uv, wz\right) \leq d\left(u,w\right) + d\left(v,z\right).
        \]
    \end{prop}

    \begin{proof}
        Per definizione, $d\left(u,w\right)$ è il costo minimo necessario per trasformare $u$ in $w$, mentre $d\left(v,z\right)$ è il costo minimo per trasformare $v$ in $z$.
        Consideriamo dunque la seguente strategia:
        \begin{enumerate}
            \item trasformiamo $u$ in $w$ mediante una sequenza ottimale di operazioni, al costo $d\left(u,w\right)$;
            \item trasformiamo $v$ in $z$ mediante una sequenza ottimale di operazioni, al costo $d\left(v,z\right)$.
        \end{enumerate}
        Applicando queste trasformazioni in sequenza, otteniamo una trasformazione da $uv$ a $wz$ con costo totale pari a $d\left(u,w\right)+d\left(v,z\right)$.

        Poiché la distanza $d\left(uv,wz\right)$ è definita come il costo minimo tra tutte le possibili trasformazioni, segue immediatamente che
        \[
            d\left(uv, wz\right) \leq d\left(u,w\right) + d\left(v,z\right). \qedhere
        \]
    \end{proof}

    \begin{prop}
        Per ogni $x', x'', y \in \Sigma^*$ vale:
        \[
            d\left(x'x'', y\right) = \min\left\{d\left(x', y'\right) + d\left(x'', y''\right) \mid y = y'y''\right\}.
        \]
    \end{prop}

    Si può inoltre individuare facilmente un limite superiore per il calcolo della distanza di edit
    tra un linguaggio $L$ ed una stringa $x \in \Sigma^*$.

    \begin{prop}
        \label{prop:distanza-limitata}
        Dato un linguaggio $L \in \Sigma^*$ ed una funzione di costo $\gamma$ esistono costanti $\alpha, \beta$ tali che
        $d\left(L, x\right) \leq \alpha \lvert x \rvert + \beta$
    \end{prop}

    \begin{proof}
        Scegliamo una stringa $z \in L$, allora $d\left(L, x\right) \leq d\left(z, x\right)$.
        Si può inoltre trasformare $z$ in $x$ concatenando $z$ con $x$ e cancellando in seguito $z$ dalla stringa risultante:

        $$\displaystyle d\left(L, x\right) \leq d\left(z, x\right) \leq \sum_{i=1}^{\lvert x \rvert} \gamma\left(\epsilon \rightarrow x_i\right) + \sum_{i=1}^{\lvert z \rvert} \gamma\left(z_i \rightarrow \epsilon\right) \leq \alpha \lvert x \rvert + \beta$$

        In questo caso, $\alpha = \max\left\{\gamma\left(\epsilon \rightarrow b\right) \mid b \in \Sigma\right\}$ e $\beta = \sum_{i=1}^{\lvert z \rvert} \gamma\left(z_i \rightarrow \epsilon\right)$
    \end{proof}

    Partendo da questo risultato, si possono individuare stringhe di $L$ che minimizzano questa distanza.

    \begin{prop}
        \label{prop:stringa-limitata}
        Dato un linguaggio $L \in \Sigma^*$ ed una funzione di costo $\gamma$ esistono costanti $\alpha', \beta'$ tali che
        $\forall{x \in \Sigma^*}, \forall{y \in L}$ tali che $d\left(L, x\right) = d\left(y, x\right)$ vale $\lvert y \rvert \leq \alpha' \lvert x \rvert + \beta'$
    \end{prop}

    \begin{proof}
        Scegliamo $y \in L$ tale che $d\left(L, x\right) = d\left(y, x\right)$ e definiamo $\lambda$ come il minimo tra i costi degli inserimenti e delle cancellazioni:

        $$ \lambda = \min\bigl\{\{\gamma\left(\epsilon \rightarrow c\right) \mid c \in \Sigma\} \cup \{\gamma\left(c \rightarrow \epsilon\right) \mid c \in \Sigma\}\bigr\}$$

        Si può affermare che per derivare una stringa $y$ da una stringa $x$, si dovranno effettuare operazioni di cancellazione o inserimento in numero pari
        ad almeno la differenza in lunghezza tra le due stringhe, segue quindi che $d\left(y, x\right) \geq \lambda\bigl|\left(\lvert y \rvert - \lvert x \rvert\right)\bigr|$.
        Si può quindi esprimere la disuguaglianza in termini di $y$:

        $$\lvert y \rvert \leq \lvert x \rvert + \frac{1}{\lambda}d\left(y, x\right)$$

        Usando la Proposizione~\ref{prop:distanza-limitata} e ricordando che $d\left(L, x\right) = d\left(y, x\right)$ otteniamo:

        $$\lvert y \rvert \leq \left(\frac{\alpha}{\lambda} + 1\right)\lvert x \rvert + \frac{\beta}{\lambda}$$
        \vspace{0.5em}

        Definendo ora $\displaystyle \alpha' \coloneqq \frac{\alpha}{\lambda} + 1$ e $\displaystyle \beta' \coloneqq \frac{\beta}{\lambda}$ la dimostrazione è conclusa.
    \end{proof}


    \section{Complessità e limiti computazionali}

    Il problema del calcolo delle distanze di edit presenta interessanti connessioni con alcune delle principali questioni aperte della complessità strutturale, in particolare con le relazioni $P \stackrel{?}{=} NP$ e $P \stackrel{?}{=} PSPACE$.

    Nella sottosezione che segue saranno date preliminarmente le definizioni delle classi di complessità di interesse per la nostra trattazione del problema del calcolo delle distanze di edit.

    \subsection{Classe $P$}

    \begin{definition}
        La classe $P$ (Polynomial time) è l'insieme dei linguaggi decisionali che possono essere risolti da una macchina di Turing deterministica in tempo polinomiale.
        Formalmente, un linguaggio $L \subseteq \{0,1\}^*$ appartiene a $P$ se esiste una macchina di Turing deterministica $M$ e un polinomio $p(n)$ tali che:
        \[
            \forall x \in \{0,1\}^*, \quad M(x) \text{ termina in tempo } O(p(|x|)) \text{ e accetta } x \iff x \in L.
        \]
        In altre parole, $P$ rappresenta la classe dei problemi decisionali per i quali esiste un algoritmo deterministico efficiente, il cui tempo di esecuzione cresce al più in modo polinomiale rispetto alla dimensione dell’input.
    \end{definition}

    \subsubsection{Esempio: ordinamento}

    Un esempio classico di problema appartenente alla classe $P$ è il problema dell'\emph{ordinamento}.
    Sia $A = (a_1, a_2, \dots, a_n)$ una sequenza di elementi appartenenti a un insieme totalmente ordinato. Il problema dell'ordinamento consiste nel determinare una permutazione $\pi$ degli indici $\{1, \dots, n\}$ tale che:
    \[
        a_{\pi(1)} \leq a_{\pi(2)} \leq \dots \leq a_{\pi(n)}.
    \]

    Dal punto di vista computazionale, il problema dell'ordinamento ammette algoritmi deterministici che operano in tempo polinomiale rispetto alla dimensione dell'input. In particolare, algoritmi come \emph{Merge Sort} e \emph{Heap Sort} risolvono il problema in tempo $O(n \log n)$.

    Poiché il tempo di esecuzione è limitato superiormente da una funzione polinomiale della dimensione dell'input, il problema dell'ordinamento (o, più formalmente, una sua versione decisionale equivalente, come la verifica che una sequenza sia ordinata) appartiene alla classe $P$.

    \subsection{Classe $PSPACE$}

    \begin{definition}
        La classe $PSPACE$ è l'insieme dei linguaggi decisionali che possono essere risolti da una macchina di Turing deterministica utilizzando una quantità di spazio polinomiale rispetto alla dimensione dell'input.

        Formalmente, un linguaggio $L \subseteq \{0,1\}^*$ appartiene a $PSPACE$ se esiste una macchina di Turing deterministica $M$ e un polinomio $p(n)$ tali che:
        \[
            \forall x \in \{0,1\}^*, \quad M(x) \text{ decide se } x \in L \text{ utilizzando al più } O(p(|x|)) \text{ celle di nastro}.
        \]

        Non vi è alcun vincolo esplicito sul tempo di esecuzione, che può essere anche esponenziale, purché lo spazio utilizzato resti polinomiale.
    \end{definition}

    Intuitivamente, $PSPACE$ rappresenta la classe dei problemi che possono essere risolti con una quantità ragionevole di memoria, anche a costo di tempi di esecuzione molto elevati.

    È noto che valgono le seguenti inclusioni:
    \[
        P \subseteq NP \subseteq PSPACE,
    \]
    mentre è tuttora aperto il problema di stabilire se tali inclusioni siano proprie.

    \subsubsection{Esempio: \textsc{QBF}}

    Un esempio classico di problema $PSPACE$-completo è il problema della \emph{soddisfacibilità booleana quantificata} (\textsc{QBF}).

    Un'istanza di \textsc{QBF} è una formula booleana in forma prenessa del tipo:
    \[
        Q_1 x_1 \, Q_2 x_2 \, \dots \, Q_n x_n \; \varphi(x_1, \dots, x_n),
    \]
    dove ciascun $Q_i \in \{\forall, \exists\}$ è un quantificatore e $\varphi$ è una formula booleana proposizionale.

    Il problema decisionale consiste nel determinare se la formula è vera.

    Ad esempio, la formula:
    \[
        \forall x \, \exists y \; (x \lor y)
    \]
    è vera, mentre:
    \[
        \exists x \, \forall y \; (x \land y)
    \]
    è falsa.

    Il problema \textsc{QBF} appartiene a $PSPACE$ poiché può essere risolto tramite una valutazione ricorsiva dei quantificatori utilizzando spazio polinomiale, memorizzando soltanto l'assegnazione corrente delle variabili.

    Inoltre, \textsc{QBF} è $PSPACE$-completo, il che implica che ogni problema in $PSPACE$ può essere ridotto ad esso tramite una riduzione calcolabile in tempo polinomiale.

    \subsection{Classe $NP$}

    \begin{definition}
        La classe $NP$ (Nondeterministic Polynomial time) è l'insieme dei linguaggi decisionali per i quali una macchina di Turing non deterministica può fornire una risposta affermativa in tempo polinomiale.
        Equivalentemente, $NP$ è l'insieme dei linguaggi per i quali una soluzione candidata può essere verificata in tempo polinomiale da una macchina deterministica.

        Formalmente, un linguaggio $L \subseteq \{0,1\}^*$ appartiene a $NP$ se esiste un linguaggio polinomiale $R(x, y)$ (detto \emph{relazione di verifica}) e un polinomio $p(n)$ tali che:
        \[
            x \in L \iff \exists y \text{ con } |y| \leq p(|x|) \text{ tale che } R(x, y) = 1.
        \]
        Il valore $y$ è detto \emph{certificato} o \emph{testimone} dell’appartenenza di $x$ a $L$.

        Intuitivamente, i problemi in $NP$ sono quelli per cui, se ci viene fornita una soluzione, possiamo verificarne la correttezza in tempo polinomiale, anche se non è noto un metodo altrettanto efficiente per trovarla.
    \end{definition}

    \subsubsection{Esempio: \textsc{SAT}}

    Un problema fondamentale appartenente alla classe $NP$ è il problema della \emph{soddisfacibilità booleana} (\textsc{SAT}).
    Un'istanza del problema \textsc{SAT} è una formula booleana $\varphi$ costruita a partire da un insieme finito di variabili proposizionali $X = \{x_1, x_2, \dots, x_n\}$ mediante l'uso dei connettivi logici $\land$, $\lor$ e $\lnot$.

    Il problema decisionale associato a \textsc{SAT} consiste nel determinare se esiste un'assegnazione di verità
    \[
        \alpha : X \rightarrow \{0,1\}
    \]
    tale che la formula $\varphi$ risulti vera sotto l'assegnazione $\alpha$.

    Ad esempio, si consideri la seguente istanza di \textsc{SAT}:
    \[
        \varphi = (x_1 \lor \lnot x_2) \land (\lnot x_1 \lor x_3) \land (x_2 \lor x_3).
    \]
    L'assegnazione
    \[
        \alpha(x_1) = 1, \quad \alpha(x_2) = 0, \quad \alpha(x_3) = 1
    \]
    soddisfa la formula, e quindi l'istanza è un'istanza affermativa del problema \textsc{SAT}.

    Il problema \textsc{SAT} appartiene alla classe $NP$ poiché, data una formula booleana $\varphi$ e una assegnazione candidata $\alpha$ (detta \emph{certificato}), è possibile verificare in tempo polinomiale rispetto alla dimensione di $\varphi$ se $\alpha$ soddisfa la formula.
    Tale verifica può essere eseguita valutando la formula in tempo lineare nel numero di simboli che la compongono.

    L'esistenza o meno di un algoritmo deterministico in tempo polinomiale per la risoluzione generale di \textsc{SAT} costituisce il nodo centrale del problema aperto $P \stackrel{?}{=} NP$.

    % TODO: cambiare stile nomi problemi, eliminare prenessa, cambiare stile titoli esempi

    \subsection{$AC^0$}

    \begin{definition}
        La classe $AC^0$ è l'insieme dei linguaggi riconoscibili da famiglie di circuiti booleani di profondità costante e dimensione polinomiale, utilizzando porte AND, OR e NOT con fan-in illimitato.

        Formalmente, un linguaggio $L \subseteq \{0,1\}^*$ appartiene ad $AC^0$ se esiste una famiglia di circuiti $\{C_n\}_{n \geq 0}$ tale che:
        \begin{itemize}
            \item per ogni $n$, il circuito $C_n$ ha $n$ input e decide l'appartenenza ad $L$ per stringhe di lunghezza $n$
            \item esiste una costante $d$ tale che ogni $C_n$ ha profondità al massimo $d$
            \item esiste un polinomio $p(n)$ tale che ogni $C_n$ ha al massimo $p(n)$ porte
            \item le porte AND e OR possono avere fan-in illimitato
        \end{itemize}
    \end{definition}

    \subsubsection{Esempio 1: \textsc{Pairwise-OR}}

    Il problema \textsc{Pairwise-OR} prevede di determinare se, in una stringa binaria, ogni coppia di bit consecutivi contiene almeno un valore $1$.

    \paragraph{Input:}

    Una stringa binaria $x = x_{1}x_{2}\ldots x_{n}$ di lunghezza $n$, con $x_i \in {0,1}$.

    \paragraph{Output:}

    $1$ se ogni coppia consecutiva $(x_i, x_{i+1})$ per $1 \leq i < n$ contiene almeno un bit uguale a $1$, ovvero se:

    \[
        f(x_1, \dots, x_n) = (x_1 \vee x_2) \wedge (x_2 \vee x_3) \wedge \dots \wedge (x_{n-1} \vee x_n)
    \]

    in caso contrario, $0$.

    \paragraph{Costruzione del circuito:}

    Per ogni coppia di bit consecutivi $(x_i, x_{i+1})$, si costruisce una porta \texttt{OR} a due ingressi che verifica la presenza di almeno un $1$:

    \[
        g_i = x_i \vee x_{i+1}, \quad 1 \leq i < n
    \]

    Le uscite di tutte le porte \texttt{OR} vengono poi collegate a una singola porta \texttt{AND}, che restituisce $1$ solo se tutte le condizioni locali sono soddisfatte:

    \[
        C_n(x) = \bigwedge_{i=1}^{n-1} (x_i \vee x_{i+1})
    \]

    Il numero di porte \texttt{OR} necessarie è pari a $n-1$, mentre la profondità complessiva del circuito è costante (pari a 2): un livello di \texttt{OR} e uno di \texttt{AND}.

    Segue che \textsc{Pairwise-OR} $\in \mathbf{AC^0}$.

    \paragraph{Esempio}

    Per $n = 4$ si ottengono le seguenti tre coppie consecutive:

    \[
        (1,2),\ (2,3),\ (3,4)
    \]

    Il corrispondente circuito è:

    \[
        C_4(x) = (x_1 \vee x_2) \wedge (x_2 \vee x_3) \wedge (x_3 \vee x_4)
    \]


    \begin{figure}[H]

        \centering

        \begin{tikzpicture}[circuit logic US, scale=0.9, every node/.style={scale=0.9}]

% OR gates

            \node[or gate, draw, logic gate inputs=nn, logic gate input sep=0.4cm] (or1) at (5,2) {};

            \node[or gate, draw, logic gate inputs=nn, logic gate input sep=0.4cm] (or2) at (5,0) {};

            \node[or gate, draw, logic gate inputs=nn, logic gate input sep=0.4cm] (or3) at (5,-2) {};


            % Inputs for or1
            \node[left=3cm of or1.input 1] (x1_1) {$x_1$};
            \node[left=3cm of or1.input 2] (x2_1) {$x_2$};
            \draw (x1_1.east) -- (or1.input 1);
            \draw (x2_1.east) -- (or1.input 2);

            % Inputs for or2
            \node[left=3cm of or2.input 1] (x2_2) {$x_2$};
            \node[left=3cm of or2.input 2] (x3_1) {$x_3$};
            \draw (x2_2.east) -- (or2.input 1);
            \draw (x3_1.east) -- (or2.input 2);

            % Inputs for or3
            \node[left=3cm of or3.input 1] (x3_2) {$x_3$};
            \node[left=3cm of or3.input 2] (x4_1) {$x_4$};
            \draw (x3_2.east) -- (or3.input 1);
            \draw (x4_1.east) -- (or3.input 2);

            % AND gate
            \node[and gate, draw, logic gate inputs=nnn, logic gate input sep=0.4cm] (and1) at (8,0) {};

            % Connections OR -> AND
            \draw (or1.output) -- ++(1,0) |- (and1.input 1);
            \draw (or2.output) -- ++(0.5,0) |- (and1.input 2);
            \draw (or3.output) -- ++(1,0) |- (and1.input 3);

            % Output
            \node[right=1cm of and1.output] (out) {$C_4(x)$};
            \draw (and1.output) -- (out);
        \end{tikzpicture}
        \caption{Circuito logico per $C_4(x)$}
        \label{fig:circuito_blocchi_sovrapposti}

    \end{figure}

    \subsubsection{Esempio 2: \textsc{Triangle}}
    Il problema \textsc{Triangle} prevede di determinare se all'interno di un grafo semplice non orientato, rappresentato mediante matrice di adiacenza,
    sia presente o meno un triangolo.

    \paragraph{Input:}
    matrice di adiacenza $A = (a_{ij})_{1 \leq i,j \leq n}$, dove $a_{ij} = 1$ se esiste un arco tra i vertici $i$ e $j$, e $a_{ij} = 0$ altrimenti.
    Poiché il grafo è non orientato, vale $a_{ij} = a_{ji}$ e $a_{ii} = 0$.

    \paragraph{Output:}
    $1$ se esiste almeno una tripla di vertici distinti $i,j,k$ tale che tra essi esistano tutti e tre gli archi,
    ovvero se $(i,j)$, $(j,k)$ e $(k,i)$ sono presenti nel grafo; in caso contrario, $0$.

    \paragraph{Costruzione del circuito:}
    Per ogni terna non ordinata di vertici distinti $\{i,j,k\}$ con $1 \leq i < j < k \leq n$, si costruisce una porta \texttt{AND} di tre ingressi che verifica la presenza di tutti e tre gli archi:
    \[
        g_{i,j,k} = a_{ij} \wedge a_{jk} \wedge a_{ki}.
    \]
    Le uscite di tutte le porte \texttt{AND} vengono poi collegate a una singola porta \texttt{OR}, che restituisce $1$ se almeno una delle condizioni è verificata:
    \[
        C_n(A) = \bigvee_{1 \leq i < j < k \leq n} (a_{ij} \wedge a_{jk} \wedge a_{ki}).
    \]
    Il numero di porte \texttt{AND} necessarie è dunque pari a $\binom{n}{3} = \frac{n(n-1)(n-2)}{6} \sim n^3$, mentre la profondità del circuito rimane costante (pari a 2), indipendentemente da $n$.
    Segue che $\textsc{Triangle} \in \mathbf{AC^0}$.

    \paragraph{Esempio}
    Per $n = 4$ si ottengono le seguenti quattro terne di vertici distinti:
    \[
        (1,2,3),\ (1,2,4),\ (1,3,4),\ (2,3,4).
    \]
    Il corrispondente circuito è:
    \[
        C_4(A) =
        (a_{12}\wedge a_{23}\wedge a_{31})
        \vee
        (a_{12}\wedge a_{24}\wedge a_{41})
        \vee
        (a_{13}\wedge a_{34}\wedge a_{41})
        \vee
        (a_{23}\wedge a_{34}\wedge a_{42}).
    \]

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[circuit logic US, scale=0.8, every node/.style={scale=0.8}]
            % AND gates
            \node[and gate, draw, logic gate inputs=nnn, logic gate input sep=0.4cm] (and1) at (6,5) {};
            \node[and gate, draw, logic gate inputs=nnn, logic gate input sep=0.4cm] (and2) at (6,3) {};
            \node[and gate, draw, logic gate inputs=nnn, logic gate input sep=0.4cm] (and3) at (6,1) {};
            \node[and gate, draw, logic gate inputs=nnn, logic gate input sep=0.4cm] (and4) at (6,-1) {};

            % Inputs per and1: a12, a23, a31
            \node[left=3cm of and1.input 1] (a12_1) {$a_{12}$};
            \node[left=3cm of and1.input 2] (a23_1) {$a_{23}$};
            \node[left=3cm of and1.input 3] (a31_1) {$a_{31}$};

            \draw (a12_1.east) -- (and1.input 1);
            \draw (a23_1.east) -- (and1.input 2);
            \draw (a31_1.east) -- (and1.input 3);

            % Inputs per and2: a12, a24, a41
            \node[left=3cm of and2.input 1] (a12_2) {$a_{12}$};
            \node[left=3cm of and2.input 2] (a24_1) {$a_{24}$};
            \node[left=3cm of and2.input 3] (a41_1) {$a_{41}$};

            \draw (a12_2.east) -- (and2.input 1);
            \draw (a24_1.east) -- (and2.input 2);
            \draw (a41_1.east) -- (and2.input 3);

            % Inputs per and3: a13, a34, a41
            \node[left=3cm of and3.input 1] (a13_1) {$a_{13}$};
            \node[left=3cm of and3.input 2] (a34_1) {$a_{34}$};
            \node[left=3cm of and3.input 3] (a41_2) {$a_{41}$};

            \draw (a13_1.east) -- (and3.input 1);
            \draw (a34_1.east) -- (and3.input 2);
            \draw (a41_2.east) -- (and3.input 3);

            % Inputs per and4: a23, a34, a42
            \node[left=3cm of and4.input 1] (a23_2) {$a_{23}$};
            \node[left=3cm of and4.input 2] (a34_2) {$a_{34}$};
            \node[left=3cm of and4.input 3] (a42_1) {$a_{42}$};

            \draw (a23_2.east) -- (and4.input 1);
            \draw (a34_2.east) -- (and4.input 2);
            \draw (a42_1.east) -- (and4.input 3);

            % OR gate
            \node[or gate, draw, logic gate inputs=nnnn, logic gate input sep=0.4cm] (or1) at (9,2) {};

            % Connessione AND -> OR
            \draw (and1.output) -- ++(1.0,0) |- (or1.input 1);
            \draw (and2.output) -- ++(0.5,0) |- (or1.input 2);
            \draw (and3.output) -- ++(0.5,0) |- (or1.input 3);
            \draw (and4.output) -- ++(1.0,0) |- (or1.input 4);

            % Output
            \node[right=1cm of or1.output] (out) {$C_4(A)$};
            \draw (or1.output) -- (out);
        \end{tikzpicture}
        \caption{Circuito logico per $C_4(A)$}
        \label{fig:circuito_c4_orizzontale}
    \end{figure}

    \clearpage

    \subsection{Implicazioni}
    Queste definizioni sono particolarmente rilevanti nel contesto delle distanze di edit, poiché permettono di caratterizzare con precisione la complessità computazionale dei problemi considerati e di stabilire connessioni con le principali questioni aperte della teoria della complessità.

    È stato dimostrato che esiste un linguaggio context-sensitive $L$ tale che la distanza di edit per $L$ è calcolabile in tempo polinomiale se e solo se $P = PSPACE$.
    Questo risultato può essere ulteriormente rafforzato: esiste un linguaggio $L$ appartenente a $\text{co-NTIME}(\log n)$ (una piccola sottoclasse di $AC^0$) la cui distanza di edit è calcolabile in tempo polinomiale se e solo se $P = NP$.

    Risultati analoghi valgono per il problema di correzione, ovvero il problema di trovare una stringa appartenente al linguaggio considerato con distanza minima dalla stringa di input: il problema di correzione per linguaggi in $NP$ (rispettivamente, per linguaggi context-sensitive) è risolvibile in tempo polinomiale se e solo se $P = NP$ (rispettivamente, $P = PSPACE$).

    \begin{theorem}
        Se $P = NP$ allora la distanza di edit di un qualunque linguaggio in $NP$ è calcolabile in tempo polinomiale
    \end{theorem}
    \begin{proof}
        Dato un linguaggio $L \in \Sigma^*$ consideriamo il problema $\Pi_L$ del decidere, dati $x \in \Sigma^*, k \geq 0$ se $d\left(L, x\right) \leq k$.
        Il problema $\Pi_L$ è risolvibile dal seguente algoritmo non deterministico:

        \vspace{0.5em}

        \begin{algorithm}
            \begin{algorithmic}[1]
                \State \textbf{input} $x, k$
                \State \textbf{indovina} una stringa $y \in \Sigma^*$, con $|y| \le \alpha'|x| + \beta'$
                \If{$y \in L$ \textbf{e} $d(y, x) \le k$}
                    \State \textbf{accetta}
                \Else
                    \State \textbf{rifiuta}
                \EndIf
            \end{algorithmic}
        \end{algorithm}

        dove $\alpha'$ e $\beta'$ sono le costanti date in ~\ref{prop:stringa-limitata}.
        Dato che la distanza tra due stringhe è calcolabile in un tempo proporzionale al prodotto delle loro lunghezze,
        non è difficile vedere che se $L \in NP$ allora $\Pi_L$ è risolvibile non deterministicamente in tempo polinomiale.
        Quindi $P = NP$ implica $\Pi_L \in P$, per ogni linguaggio $L \in NP$.

        Si può ricavare un algoritmo che calcola $d\left(L, x\right)$ stabilendo l'appartenenza a $\Pi_L$ di coppie $\left(x, k\right)$, per valori crescenti di $k$.
        Ricordando che $d\left(L, x\right) \leq \alpha\lvert x\rvert + \beta$, date delle costanti adatte $\alpha$ e $\beta$, si nota che se $\Pi_L \in P$ allora questo algoritmo opera in tempo polinomiale deterministico.
    \end{proof}

    \begin{theorem}[Teorema 3.2 -- Edit distance e PSPACE]
        Esiste un linguaggio context-sensitive \(L\) tale che il problema di calcolare la distanza di edit verso \(L\) è risolvibile in tempo polinomiale se e solo se \(P = PSPACE\).
    \end{theorem}

    \begin{proof}[Dimostrazione]
        Come dimostrato da Karp~\cite{Karp 1}, esiste un linguaggio context-sensitive \(L\) che è \(PSPACE\)-completo rispetto alle riduzioni in tempo polinomiale.

        Supponiamo che la distanza di edit verso \(L\) sia calcolabile in tempo polinomiale. Allora, dato un input \(x\), possiamo decidere in tempo polinomiale se \(x \in L\) osservando che
        \[
            x \in L \iff d(x,L) = 0.
        \]
        Ne consegue che \(L \in P\). Essendo \(L\) \(PSPACE\)-completo, segue che
        \[
            PSPACE \subseteq P,
        \]
        e quindi \(P = PSPACE\).

        La direzione inversa è conseguenza del Teorema 3.1: se \(P = PSPACE\), allora ogni problema \(PSPACE\)-completo, e in particolare \(L\), ha una distanza di edit calcolabile in tempo polinomiale.
    \end{proof}

    Ci focalizziamo adesso sulla classe $\text{co-NTIME}(\log n)$.
    Come mostrato da Pighizzini[2] la distanza di edit per i problemi in questa classe è calcolabile in tempo polinomiale se e solo se $P = NP$.
    La dimostrazione fa uso del linguaggio $L_{acc}$ che, dato un insieme $L$ $NP$-completo, accettato da una macchina di Turing non deterministica $M$ in tempo polinomiale,
    rappresenta le stringhe che codificano computazioni accettanti di $M$, formalmente:

    \[
        L_{\text{acc}} = \left\{
                             x \# \omega_0 \# \omega_1 \# \dots \# \omega_{p(n)}
                             \;\middle|\;
                             \begin{array}{l}
                                 x \in \Sigma^*,\; n = |x|, \# \notin \Sigma \cup \Gamma \cup Q, \\[4pt]
                                 \omega_i \text{ è una configurazione di } M, \\[4pt]
                                 \lvert \omega_i \rvert = p'(n), \omega_i \in \Gamma \cup Q \\[4pt]
                                 \omega_0 \text{ è la configurazione iniziale di } M \text{ su input } x, \\[4pt]
                                 \forall j = 1, \dots, p(n),\; \omega_{j-1} \vdash_M \omega_j, \\[4pt]
                                 \omega_{p(n)} \text{ è una configurazione accettante}
                             \end{array}
        \right\}
    \]


    \chapter{Conclusioni}

    Osservazioni finali e possibili sviluppi futuri\ldots

    \clearpage
    \begin{thebibliography}{9}

        \bibitem{okhotin-salomaa}
        Alexander Okhotin, Kai Salomaa (2014),
        \textit{Complexity of Input-Driven Pushdown Automata}.
        \bibitem{pighizzini}
        Giovanni Pighizzini (2001),
        \textit{How Hard Is Computing the Edit Distance?}.
        \bibitem{Karp 1}
        Karp, R. (1972),
        \textit{Reducibility among combinatorial problems, in “Complexity of Computer Computations”}.
        \bibitem{Chandra}
        Chandra, A., Kozen, D., and Stockmeyer, L. (1981),
        \textit{Alternation, J. Assoc. Comput. Mach. 28, 114–133}.

% Aggiungere altre fonti

    \end{thebibliography}

\end{document}
